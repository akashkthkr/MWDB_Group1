'''
 Task 4: Locality-Sensitive Hashing
 
– Implement a Locality Sensitive Hashing (LSH) tool, which takes as input (a) the number of layers, L, (b) the number of hashes per layer, κ, 
and (c) a set of vectors (generated by other tasks) as input and creates an in-memory index structure containing the given set of vectors. See
 ”Near-Optimal Hashing Algorithms for the Approximate Nearest Neighbor in High Dimensions” (by Alexandr Andoni and Piotr Indyk). Communications of the ACM, vol. 51, no. 1, 2008, pp. 117-122.
 
– Implement similar image search using this index structure:
 
∗ given a folder of images and one of the three feature models, the images are stored in an LSH data structure (the program also outputs the size of the index structure in bytes), and
 
∗ given image and t, the tool outputs the t most similar images; it also outputs
 
· the numbers of buckets searched as well as the unique and overall number of images considered
 
· false positive and miss rates.

'''
'''
Inputs:
(a) the number of layers
(b) the number of hashes per layer
κ, and (c) a set of vectors (generated by other tasks) 

Intermediary:
In memory index structure LSH to hold the set of vectors 
'''
'''
Second Part:
Image Search implementation using index struct
Inputs:
Folder of images : 
3 Feature models(Hog,elbp etc)
Store image in LSH Data Struct
Test Case :
Given a image and t (5 most similar)
'''
'''
Output:
Display the size of the files in bytes : outputs the t most similar images
Number of buckets searched as well as unique overall number of images considered
False positive and miss rates
'''

import json
import pickle
from pathlib import Path
from constants.Constants_Phase3 import OUTPUTS_PATH
import os
import numpy as np


def euclidean_dist_square(x, y):
    x = np.array(x)
    y = np.array(y)
    return np.linalg.norm(x - y)


class Lsh(object):

    def __init__(self, number_of_hashes_per_layer, number_of_features, num_layers=2):
        self.num_layers = num_layers
        self.number_of_hashes_per_layer = number_of_hashes_per_layer
        self.number_of_features = number_of_features
        self.random_planes = [np.random.randn(self.number_of_hashes_per_layer, self.number_of_features)
                              for _ in range(self.num_layers)]
        self.layers = [dict() for i in range(self.num_layers)]

    def get_combined_hash_value(self, planes, input_point):
        input_point = np.array(input_point)
        projections = planes.dot(input_point)
        return "".join(['1' if i > 0 else '0' for i in projections])

    def add_to_index_structure(self, input_feature, image_id=''):
        value = tuple(input_feature)
        for i, layer in enumerate(self.layers):
            layer.setdefault(self.get_combined_hash_value(self.random_planes[i], input_feature), []).append(
                (value, image_id))

    def query(self, feature, num_results=None):
        image_hits = set()
        calculate_distance = euclidean_dist_square
        buckets = set()
        for i, layer in enumerate(self.layers):
            combined_hash_value = self.get_combined_hash_value(self.random_planes[i], feature)
            buckets.add(combined_hash_value)
            image_hits.update(layer.get(combined_hash_value, []))

        while len(image_hits) < num_results:
            for i, layer in enumerate(self.layers):
                combined_hash_value = self.get_combined_hash_value(self.random_planes[i], feature)
                image_hits.update(layer.get(combined_hash_value, []))
                combined_hash_value = self.get_combined_hash_value(self.random_planes[i], feature)
                image_hits.update(layer.get(combined_hash_value, []))

        image_hits = [(hit_tuple[1], calculate_distance(feature, np.asarray(hit_tuple[0])))
                      for hit_tuple in image_hits]
        image_hits.sort(key=lambda v: v[1])

        result = image_hits[:num_results] if num_results else image_hits
        return result, len(image_hits), len(set(image_hits)), len(buckets)

    def save_to_json(self, filename, object_to_store):
        with open(filename, 'w') as json_file:
            json.dump(object_to_store, json_file)

    def save_to_pickle(self, filename, object_to_store):
        with open(filename, 'wb') as pickle_file:
            pickle.dump(object_to_store, pickle_file)

    def save_result(self, result):
        output_folder = OUTPUTS_PATH
        self.save_to_json(output_folder + "lsh_data.json", result)
